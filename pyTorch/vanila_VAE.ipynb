{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import  print_function\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_npz):\n",
    "        x_npz = np.load(data_npz)\n",
    "        x_ww = x_npz['arr_0']\n",
    "        self.len = x_ww.shape[0]\n",
    "        self.dim = x_ww.shape[1]\n",
    "        self.data = torch.from_numpy(x_ww)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datset = MyDataset('../x_ww_bw_50176_pre-processed.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(datset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=datset, batch_size=32, sampler=train_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=datset, batch_size=32, sampler=val_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(50176, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 50176)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.rand_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1,50176))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 50176), reduction='sum')\n",
    "    \n",
    "    KLD = -0.5* torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_func(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('train epoch: {} [{}/{} ({:.2f}%)]\\tLoss:{:.3f}'.format(\n",
    "                    epoch, batch_idx*len(data), len(train_loader.dataset),\n",
    "                    100.* batch_idx/len(train_loader), loss.item()/len(data)))\n",
    "            \n",
    "#        print('=====> Epoch:{} Avarage Loss: {:.4f}'.format(\n",
    "#                epoch, train_loss/ len(train_loader.dataset)))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(train_loader, 0):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_func(recon_batch, data, mu, logvar).item()\n",
    "            if batch_idx ==0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data.view(32,1,224,224)[:n], recon_batch.view(32,1,224,224)[:n]])\n",
    "                save_image(comparison.cpu(), 'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('=====> test set loss:{:.4f}'.format(test_loss))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [0/49993 (0.00%)]\tLoss:34917.445\n",
      "train epoch: 1 [3200/49993 (8.00%)]\tLoss:11030.261\n",
      "train epoch: 1 [6400/49993 (16.00%)]\tLoss:9162.209\n",
      "train epoch: 1 [9600/49993 (24.00%)]\tLoss:9211.058\n",
      "train epoch: 1 [12800/49993 (32.00%)]\tLoss:8712.184\n",
      "train epoch: 1 [16000/49993 (40.00%)]\tLoss:8615.823\n",
      "train epoch: 1 [19200/49993 (48.00%)]\tLoss:8249.754\n",
      "train epoch: 1 [22400/49993 (56.00%)]\tLoss:9466.723\n",
      "train epoch: 1 [25600/49993 (64.00%)]\tLoss:8768.545\n",
      "train epoch: 1 [28800/49993 (72.00%)]\tLoss:8393.656\n",
      "train epoch: 1 [32000/49993 (80.00%)]\tLoss:8744.381\n",
      "train epoch: 1 [35200/49993 (88.00%)]\tLoss:9029.450\n",
      "train epoch: 1 [38400/49993 (96.00%)]\tLoss:8979.657\n",
      "=====> test set loss:6675.1595\n",
      "train epoch: 2 [0/49993 (0.00%)]\tLoss:8066.669\n",
      "train epoch: 2 [3200/49993 (8.00%)]\tLoss:8164.577\n",
      "train epoch: 2 [6400/49993 (16.00%)]\tLoss:8207.202\n",
      "train epoch: 2 [9600/49993 (24.00%)]\tLoss:7682.562\n",
      "train epoch: 2 [12800/49993 (32.00%)]\tLoss:7999.230\n",
      "train epoch: 2 [16000/49993 (40.00%)]\tLoss:8166.006\n",
      "train epoch: 2 [19200/49993 (48.00%)]\tLoss:8212.343\n",
      "train epoch: 2 [22400/49993 (56.00%)]\tLoss:8557.560\n",
      "train epoch: 2 [25600/49993 (64.00%)]\tLoss:8149.341\n",
      "train epoch: 2 [28800/49993 (72.00%)]\tLoss:9171.840\n",
      "train epoch: 2 [32000/49993 (80.00%)]\tLoss:8062.637\n",
      "train epoch: 2 [35200/49993 (88.00%)]\tLoss:7849.797\n",
      "train epoch: 2 [38400/49993 (96.00%)]\tLoss:8526.378\n",
      "=====> test set loss:6530.3276\n",
      "train epoch: 3 [0/49993 (0.00%)]\tLoss:8205.427\n",
      "train epoch: 3 [3200/49993 (8.00%)]\tLoss:7474.022\n",
      "train epoch: 3 [6400/49993 (16.00%)]\tLoss:8434.531\n",
      "train epoch: 3 [9600/49993 (24.00%)]\tLoss:7973.713\n",
      "train epoch: 3 [12800/49993 (32.00%)]\tLoss:7603.394\n",
      "train epoch: 3 [16000/49993 (40.00%)]\tLoss:8149.106\n",
      "train epoch: 3 [19200/49993 (48.00%)]\tLoss:8031.358\n",
      "train epoch: 3 [22400/49993 (56.00%)]\tLoss:8301.124\n",
      "train epoch: 3 [25600/49993 (64.00%)]\tLoss:7493.250\n",
      "train epoch: 3 [28800/49993 (72.00%)]\tLoss:8188.675\n",
      "train epoch: 3 [32000/49993 (80.00%)]\tLoss:7756.315\n",
      "train epoch: 3 [35200/49993 (88.00%)]\tLoss:8441.506\n",
      "train epoch: 3 [38400/49993 (96.00%)]\tLoss:8180.587\n",
      "=====> test set loss:6467.3318\n",
      "train epoch: 4 [0/49993 (0.00%)]\tLoss:7421.197\n",
      "train epoch: 4 [3200/49993 (8.00%)]\tLoss:7643.154\n",
      "train epoch: 4 [6400/49993 (16.00%)]\tLoss:8414.022\n",
      "train epoch: 4 [9600/49993 (24.00%)]\tLoss:7967.453\n",
      "train epoch: 4 [12800/49993 (32.00%)]\tLoss:8200.942\n",
      "train epoch: 4 [16000/49993 (40.00%)]\tLoss:7746.373\n",
      "train epoch: 4 [19200/49993 (48.00%)]\tLoss:8302.564\n",
      "train epoch: 4 [22400/49993 (56.00%)]\tLoss:7776.653\n",
      "train epoch: 4 [25600/49993 (64.00%)]\tLoss:7512.536\n",
      "train epoch: 4 [28800/49993 (72.00%)]\tLoss:8235.549\n",
      "train epoch: 4 [32000/49993 (80.00%)]\tLoss:8951.412\n",
      "train epoch: 4 [35200/49993 (88.00%)]\tLoss:8470.466\n",
      "train epoch: 4 [38400/49993 (96.00%)]\tLoss:8006.664\n",
      "=====> test set loss:6430.4350\n",
      "train epoch: 5 [0/49993 (0.00%)]\tLoss:8176.296\n",
      "train epoch: 5 [3200/49993 (8.00%)]\tLoss:7761.582\n",
      "train epoch: 5 [6400/49993 (16.00%)]\tLoss:8140.869\n",
      "train epoch: 5 [9600/49993 (24.00%)]\tLoss:7322.099\n",
      "train epoch: 5 [12800/49993 (32.00%)]\tLoss:8053.499\n",
      "train epoch: 5 [16000/49993 (40.00%)]\tLoss:8403.576\n",
      "train epoch: 5 [19200/49993 (48.00%)]\tLoss:7714.226\n",
      "train epoch: 5 [22400/49993 (56.00%)]\tLoss:8305.925\n",
      "train epoch: 5 [25600/49993 (64.00%)]\tLoss:8014.438\n",
      "train epoch: 5 [28800/49993 (72.00%)]\tLoss:8299.542\n",
      "train epoch: 5 [32000/49993 (80.00%)]\tLoss:7846.812\n",
      "train epoch: 5 [35200/49993 (88.00%)]\tLoss:7845.910\n",
      "train epoch: 5 [38400/49993 (96.00%)]\tLoss:8180.098\n",
      "=====> test set loss:6420.9652\n",
      "train epoch: 6 [0/49993 (0.00%)]\tLoss:7756.626\n",
      "train epoch: 6 [3200/49993 (8.00%)]\tLoss:8204.049\n",
      "train epoch: 6 [6400/49993 (16.00%)]\tLoss:8107.580\n",
      "train epoch: 6 [9600/49993 (24.00%)]\tLoss:8346.090\n",
      "train epoch: 6 [12800/49993 (32.00%)]\tLoss:8025.855\n",
      "train epoch: 6 [16000/49993 (40.00%)]\tLoss:7947.833\n",
      "train epoch: 6 [19200/49993 (48.00%)]\tLoss:8162.469\n",
      "train epoch: 6 [22400/49993 (56.00%)]\tLoss:7824.393\n",
      "train epoch: 6 [25600/49993 (64.00%)]\tLoss:8241.970\n",
      "train epoch: 6 [28800/49993 (72.00%)]\tLoss:8132.942\n",
      "train epoch: 6 [32000/49993 (80.00%)]\tLoss:8140.831\n",
      "train epoch: 6 [35200/49993 (88.00%)]\tLoss:8374.500\n",
      "train epoch: 6 [38400/49993 (96.00%)]\tLoss:7557.883\n",
      "=====> test set loss:6402.3704\n",
      "train epoch: 7 [0/49993 (0.00%)]\tLoss:7369.010\n",
      "train epoch: 7 [3200/49993 (8.00%)]\tLoss:8256.927\n",
      "train epoch: 7 [6400/49993 (16.00%)]\tLoss:7831.875\n",
      "train epoch: 7 [9600/49993 (24.00%)]\tLoss:7780.244\n",
      "train epoch: 7 [12800/49993 (32.00%)]\tLoss:7620.914\n",
      "train epoch: 7 [16000/49993 (40.00%)]\tLoss:8495.824\n",
      "train epoch: 7 [19200/49993 (48.00%)]\tLoss:8356.850\n",
      "train epoch: 7 [22400/49993 (56.00%)]\tLoss:7585.032\n",
      "train epoch: 7 [25600/49993 (64.00%)]\tLoss:7988.175\n",
      "train epoch: 7 [28800/49993 (72.00%)]\tLoss:8503.187\n",
      "train epoch: 7 [32000/49993 (80.00%)]\tLoss:8795.547\n",
      "train epoch: 7 [35200/49993 (88.00%)]\tLoss:8139.207\n",
      "train epoch: 7 [38400/49993 (96.00%)]\tLoss:8327.317\n",
      "=====> test set loss:6382.9973\n",
      "train epoch: 8 [0/49993 (0.00%)]\tLoss:7836.481\n",
      "train epoch: 8 [3200/49993 (8.00%)]\tLoss:8702.045\n",
      "train epoch: 8 [6400/49993 (16.00%)]\tLoss:7828.811\n",
      "train epoch: 8 [9600/49993 (24.00%)]\tLoss:8279.897\n",
      "train epoch: 8 [12800/49993 (32.00%)]\tLoss:7828.470\n",
      "train epoch: 8 [16000/49993 (40.00%)]\tLoss:8135.728\n",
      "train epoch: 8 [19200/49993 (48.00%)]\tLoss:8380.697\n",
      "train epoch: 8 [22400/49993 (56.00%)]\tLoss:8821.896\n",
      "train epoch: 8 [25600/49993 (64.00%)]\tLoss:8091.553\n",
      "train epoch: 8 [28800/49993 (72.00%)]\tLoss:6955.910\n",
      "train epoch: 8 [32000/49993 (80.00%)]\tLoss:7832.824\n",
      "train epoch: 8 [35200/49993 (88.00%)]\tLoss:8166.656\n",
      "train epoch: 8 [38400/49993 (96.00%)]\tLoss:8812.660\n",
      "=====> test set loss:6362.5681\n",
      "train epoch: 9 [0/49993 (0.00%)]\tLoss:7758.840\n",
      "train epoch: 9 [3200/49993 (8.00%)]\tLoss:8361.920\n",
      "train epoch: 9 [6400/49993 (16.00%)]\tLoss:7321.509\n",
      "train epoch: 9 [9600/49993 (24.00%)]\tLoss:8553.919\n",
      "train epoch: 9 [12800/49993 (32.00%)]\tLoss:7723.847\n",
      "train epoch: 9 [16000/49993 (40.00%)]\tLoss:7903.422\n",
      "train epoch: 9 [19200/49993 (48.00%)]\tLoss:7900.907\n",
      "train epoch: 9 [22400/49993 (56.00%)]\tLoss:8228.347\n",
      "train epoch: 9 [25600/49993 (64.00%)]\tLoss:7955.279\n",
      "train epoch: 9 [28800/49993 (72.00%)]\tLoss:8028.195\n",
      "train epoch: 9 [32000/49993 (80.00%)]\tLoss:7525.628\n",
      "train epoch: 9 [35200/49993 (88.00%)]\tLoss:7573.930\n",
      "train epoch: 9 [38400/49993 (96.00%)]\tLoss:8504.965\n",
      "=====> test set loss:6368.3824\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,10):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(32, 20).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(32,1,224,224), 'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
